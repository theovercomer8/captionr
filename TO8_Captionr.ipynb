{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNx4DEbu3zPDfMJLe/t9+1n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/theovercomer8/captionr/blob/main/TO8_Captionr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install Dependencies\n",
        "#@markdown Installs requirements.txt and imports CLIP data\n",
        "import os\n",
        "\n",
        "if os.path.exists('/content/data'):\n",
        "  !rm -rf /content/data\n",
        "\n",
        "\n",
        "if os.path.exists('/content/requirements.txt'):\n",
        "  !rm /content/requirements.txt\n",
        "\n",
        "!mkdir /content/data\n",
        "%cd /content/data\n",
        "\n",
        "\n",
        "if not os.path.exists('/content/dataset'):\n",
        "  !mkdir /content/dataset\n",
        "\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/breadboard/main/utils/data/artists.txt\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/breadboard/main/utils/data/flavors.txt\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/breadboard/main/utils/data/movements.txt\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/breadboard/main/utils/data/mediums.txt\n",
        "\n",
        "%cd /content\n",
        "!wget https://raw.githubusercontent.com/theovercomer8/breadboard/main/requirements.txt\n",
        "\n",
        "!pip install -r requirements.txt\n",
        "!pip install pillow\n"
      ],
      "metadata": {
        "id": "ie6oPstv4Xx5",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Login to Huggingface hub\n",
        "from huggingface_hub import login\n",
        "%store -r\n",
        "\n",
        "#@markdown Login to Huggingface hub\n",
        "#@markdown 1. You need a Huggingface account.\n",
        "#@markdown 2. To create a huggingface token, go to https://huggingface.co/settings/tokens, then create a new token or copy an available token with the `Write` role.\n",
        "write_token = \"\" #@param {type:\"string\"}\n",
        "login(write_token, add_to_git_credential=True)\n",
        "\n",
        "%store write_token\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "P8Zrmzbu5gEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download and Extract Zip (.zip)\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "%store -r\n",
        "\n",
        "#@markdown ### Define Zipfile URL or Zipfile Path\n",
        "zipfile_url_or_path = \"\" #@param {'type': 'string'}\n",
        "zipfile_dst = str(root_dir)+\"/zip_file.zip\"\n",
        "extract_to = \"/content/dataset\" #@param {'type': 'string'}\n",
        "\n",
        "if extract_to != \"\":\n",
        "  os.makedirs(extract_to, exist_ok=True)\n",
        "else:\n",
        "  extract_to = \"/content/dataset\"\n",
        "\n",
        "#@markdown This will ignore `extract_to` path and automatically extracting to `train_data_dir`\n",
        "is_dataset = False #@param{'type':'boolean'}\n",
        "\n",
        "#@markdown Tick this if you want to extract all files directly to `extract_to` folder, and automatically delete the zip to save the memory\n",
        "auto_unzip_and_delete = False #@param{'type':'boolean'}\n",
        "\n",
        "dirname = os.path.dirname(zipfile_dst)\n",
        "basename = os.path.basename(zipfile_dst)\n",
        "\n",
        "try:\n",
        "  if zipfile_url_or_path.startswith(\"/content\"):\n",
        "    zipfile_dst = zipfile_url_or_path\n",
        "    if auto_unzip_and_delete == False:\n",
        "      if is_dataset:\n",
        "        extract_to = \"/content/dataset\"\n",
        "      !unzip -j {zipfile_dst} -d \"{extract_to}\"\n",
        "  elif zipfile_url_or_path.startswith(\"https://drive.google.com\"):\n",
        "    !gdown --fuzzy  {zipfile_url_or_path}\n",
        "  elif zipfile_url_or_path.startswith(\"magnet:?\"):\n",
        "    !aria2c --summary-interval=10 -c -x 10 -k 1M -s 10 {zipfile_url_or_path}\n",
        "  elif zipfile_url_or_path.startswith(\"https://huggingface.co/\"):\n",
        "    if '/blob/' in zipfile_url_or_path:\n",
        "      zipfile_url_or_path = zipfile_url_or_path.replace('/blob/', '/resolve/')\n",
        "\n",
        "    hf_token = write_token\n",
        "    user_header = f\"\\\"Authorization: Bearer {hf_token}\\\"\"\n",
        "    !aria2c --console-log-level=error --summary-interval=10 --header={user_header} -c -x 16 -k 1M -s 16 -d {dirname} -o {basename} {zipfile_url_or_path}\n",
        "  else:\n",
        "    !aria2c --console-log-level=error --summary-interval=10 -c -x 16 -k 1M -s 16 -d {dirname} -o {basename} {zipfile_url_or_path}\n",
        "\n",
        "except Exception as e:\n",
        "  print(\"An error occurred while downloading the file:\", e)\n",
        "\n",
        "if is_dataset:\n",
        "  extract_to = '/content/dataset'\n",
        "\n",
        "if auto_unzip_and_delete:\n",
        "  !unzip -j {zipfile_dst} -d \"{extract_to}\"\n",
        "\n",
        "  path_obj = Path(zipfile_dst)\n",
        "  zipfile_name = path_obj.parts[-1]\n",
        "  \n",
        "  if os.path.isdir(zipfile_dst):\n",
        "    print(\"\\nThis zipfile doesn't exist or has been deleted \\n\")\n",
        "  else:\n",
        "    os.remove(zipfile_dst)\n",
        "    print(f\"\\n{zipfile_name} has been deleted\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Do6uhfD85t91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnmjTJVy0aNL",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Caption Wizard\n",
        "\n",
        "from json import JSONDecoder, JSONEncoder\n",
        "import sys\n",
        "import getopt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import hashlib\n",
        "import inspect\n",
        "import math\n",
        "import numpy as np\n",
        "import open_clip\n",
        "import pickle\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from blip.models.blip import blip_decoder, BLIP_Decoder\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "from   IPython.core.display import display, HTML\n",
        "import gc\n",
        "\n",
        "\n",
        "# All objects we find\n",
        "json_found = []  \n",
        "# raw_decode expects byte1 to be part of a JSON, so remove whitespace from left\n",
        "# stdin = sys.stdin.read().lstrip()\n",
        "decoder = JSONDecoder()\n",
        "encoder = JSONEncoder()\n",
        "\n",
        "#@markdown Folder to scan for images\n",
        "folder_path = '/content/dataset' #@param {type:\"string\"}\n",
        "#@markdown Folder to output captions. Captioned will be named the same as the input image with a .txt extension\n",
        "output_path = '/content/dataset' #@param {type:\"string\"}\n",
        "git_pass = True #@param {type:\"boolean\"}\n",
        "#@markdown If GIT Pass enabled, fail the caption and move on to BLIP pass if caption contains any of the following phrases (comma delimted)\n",
        "git_fail_phrases = 'a sign that says,writing that says,that says,with the word' #@param {type:\"string\"}\n",
        "#@markdown Perform a BLIP caption pass\n",
        "blip_pass = True #@param {type:\"boolean\"}\n",
        "cap_length = 150 #@param {type: \"slider\", min: 0, max: 400}\n",
        "#@markdown Action to take for existing caption files\n",
        "existing = 'skip' #@param [ 'skip', 'ignore', 'copy', 'prepend', 'append']\n",
        "clip_beams = 20 #@param {type: \"slider\", min: 1, max: 20}\n",
        "clip_min = 30 #@param {type: \"slider\", min: 5, max: 75}\n",
        "clip_max = 75 #@param {type: \"slider\", min: 5, max: 75}\n",
        "#@markdown If checked, will use new Coco clip\n",
        "clip_v2 = True #@param {type:\"boolean\"}\n",
        "clip_use_flavor = True #@param {type:\"boolean\"}\n",
        "clip_max_flavors = 10 #@param {type: \"slider\", min: 1, max: 10}\n",
        "clip_use_artist = False #@param {type:\"boolean\"}\n",
        "clip_use_medium = False #@param {type:\"boolean\"}\n",
        "clip_use_movement = False #@param {type:\"boolean\"}\n",
        "clip_use_trending = False  #@param {type:\"boolean\"}\n",
        "ignore_tags = '' #@param {type:\"string\"}\n",
        "#@markdown Find/replace in caption. Set `replace_class` to true to use. Will replace any instances of `sub_class` with `sub_name`\n",
        "replace_class = False #@param {type:\"boolean\"}\n",
        "sub_class = '' #@param {type:\"string\"}\n",
        "sub_name = '' #@param {type:\"string\"}\n",
        "#@markdown Tag the caption with the containing folder. Useful when using nested folder structure. Can tag up to `folder_tag_levels` deep\n",
        "folder_tag = False #@param {type:\"boolean\"}\n",
        "folder_tag_levels = 1 #@param {type: \"slider\", min: 1, max: 10}\n",
        "uniquify_tags = True #@param {type:\"boolean\"}\n",
        "#@markdown Uncheck `write_to_file` to just perform a preview run\n",
        "write_to_file = True #@param {type:\"boolean\"}\n",
        "#@markdown Read caption from filename if caption file does not exist\n",
        "use_filename = False #@param {type:\"boolean\"}\n",
        "\n",
        "device = \"cuda\" #if torch.cuda.is_available() else \"cpu\"\n",
        "processor = None\n",
        "model = None\n",
        "\n",
        "def get_parent_folder(filepath, levels=1):\n",
        "    common = os.path.split(filepath)[0]\n",
        "    paths = []\n",
        "    for i in range(int(levels)):\n",
        "        split = os.path.split(common)\n",
        "        common = split[0]\n",
        "        paths.append(split[1])\n",
        "    return paths\n",
        "\n",
        "\n",
        "def git_caption(img):\n",
        "    pixel_values = processor(images=img, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "    pixel_values = pixel_values.to(device)\n",
        "    generated_ids = model.generate(pixel_values=pixel_values, max_length=150)\n",
        "    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return generated_caption\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "BLIP_MODELS = {\n",
        "    'base': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth',\n",
        "    'large': 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth'\n",
        "}\n",
        "\n",
        "@dataclass \n",
        "class Config:\n",
        "    # models can optionally be passed in directly\n",
        "    blip_model: BLIP_Decoder = None\n",
        "    clip_model = None\n",
        "    clip_preprocess = None\n",
        "\n",
        "    # blip settings\n",
        "    blip_image_eval_size: int = 384\n",
        "    blip_model_type: str = 'large' # choose between 'base' or 'large'\n",
        "    blip_offload: bool = False\n",
        "\n",
        "    # clip settings\n",
        "    clip_model_name: str = 'ViT-L-14/openai' if not clip_v2 else 'coca_ViT-L-14/mscoco_finetuned_laion2B-s13B-b90k'\n",
        "    clip_model_path: str = None\n",
        "\n",
        "    # interrogator settings\n",
        "    cache_path: str = 'cache'\n",
        "    chunk_size: int = 2048\n",
        "    data_path: str = '/content/data'\n",
        "    device: str = (\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    flavor_intermediate_count: int = 2048\n",
        "    quiet: bool = False # when quiet progress bars are not shown\n",
        "\n",
        "class Interrogator():\n",
        "    def __init__(self, config: Config):\n",
        "        self.config = config\n",
        "        self.device = device\n",
        "\n",
        "        if blip_pass:\n",
        "            if config.blip_model is None:\n",
        "                if not config.quiet:\n",
        "                    print(\"Loading BLIP model...\")\n",
        "                blip_path = os.path.dirname(inspect.getfile(blip_decoder))\n",
        "                configs_path = os.path.join(os.path.dirname(blip_path), 'configs')\n",
        "                med_config = os.path.join(configs_path, 'med_config.json')\n",
        "                blip_model = blip_decoder(\n",
        "                    pretrained=BLIP_MODELS[config.blip_model_type],\n",
        "                    image_size=config.blip_image_eval_size, \n",
        "                    vit=config.blip_model_type, \n",
        "                    med_config=med_config\n",
        "                )\n",
        "                blip_model.eval()\n",
        "                blip_model = blip_model.to(config.device)\n",
        "                self.blip_model = blip_model\n",
        "            else:\n",
        "                self.blip_model = config.blip_model\n",
        "\n",
        "        if clip_use_movement or clip_use_artist or clip_use_flavor or clip_use_medium or clip_use_trending:\n",
        "            self.load_clip_model()\n",
        "\n",
        "    def load_clip_model(self):\n",
        "        start_time = time.time()\n",
        "        config = self.config\n",
        "\n",
        "        if config.clip_model is None:\n",
        "            if not config.quiet:\n",
        "                print(\"Loading CLIP model...\")\n",
        "\n",
        "            clip_model_name, clip_model_pretrained_name = config.clip_model_name.split('/', 2)\n",
        "            self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms(\n",
        "                clip_model_name, \n",
        "                pretrained=clip_model_pretrained_name, \n",
        "                precision='fp16' if config.device == 'cuda' else 'fp32',\n",
        "                device=config.device,\n",
        "                jit=False,\n",
        "                cache_dir=config.clip_model_path\n",
        "            )\n",
        "            self.clip_model.to(config.device).eval()\n",
        "        else:\n",
        "            self.clip_model = config.clip_model\n",
        "            self.clip_preprocess = config.clip_preprocess\n",
        "        self.tokenize = open_clip.get_tokenizer(clip_model_name)\n",
        "\n",
        "        sites = ['Artstation', 'behance', 'cg society', 'cgsociety', 'deviantart', 'dribble', 'flickr', 'instagram', 'pexels', 'pinterest', 'pixabay', 'pixiv', 'polycount', 'reddit', 'shutterstock', 'tumblr', 'unsplash', 'zbrush central']\n",
        "        trending_list = [site for site in sites]\n",
        "        trending_list.extend([\"trending on \"+site for site in sites])\n",
        "        trending_list.extend([\"featured on \"+site for site in sites])\n",
        "        trending_list.extend([site+\" contest winner\" for site in sites])\n",
        "\n",
        "        raw_artists = _load_list(config.data_path, 'artists.txt')\n",
        "        artists = [f\"by {a}\" for a in raw_artists]\n",
        "        artists.extend([f\"inspired by {a}\" for a in raw_artists])\n",
        "\n",
        "        if clip_use_artist:\n",
        "            self.artists = LabelTable(artists, \"artists\", self.clip_model, self.tokenize, config)\n",
        "        \n",
        "        if clip_use_flavor:\n",
        "            self.flavors = LabelTable(_load_list(config.data_path, 'flavors.txt'), \"flavors\", self.clip_model, self.tokenize, config)\n",
        "        \n",
        "        if clip_use_medium:\n",
        "            self.mediums = LabelTable(_load_list(config.data_path, 'mediums.txt'), \"mediums\", self.clip_model, self.tokenize, config)\n",
        "        \n",
        "        if clip_use_movement:\n",
        "            self.movements = LabelTable(_load_list(config.data_path, 'movements.txt'), \"movements\", self.clip_model, self.tokenize, config)\n",
        "        \n",
        "        if clip_use_trending:\n",
        "            self.trendings = LabelTable(trending_list, \"trendings\", self.clip_model, self.tokenize, config)\n",
        "\n",
        "        end_time = time.time()\n",
        "        if not config.quiet:\n",
        "            print(f\"Loaded CLIP model and data in {end_time-start_time:.2f} seconds.\")\n",
        "\n",
        "    def generate_blip_caption(self, pil_image: Image) -> str:\n",
        "        if self.config.blip_offload:\n",
        "            self.blip_model = self.blip_model.to(self.device)\n",
        "        size = self.config.blip_image_eval_size\n",
        "        gpu_image = transforms.Compose([\n",
        "            transforms.Resize((size, size), interpolation=InterpolationMode.BICUBIC),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
        "        ])(pil_image).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            caption = self.blip_model.generate(\n",
        "                gpu_image, \n",
        "                sample=False, \n",
        "                num_beams=clip_beams, \n",
        "                max_length=clip_max, \n",
        "                min_length=clip_min\n",
        "            )\n",
        "        if self.config.blip_offload:\n",
        "            self.blip_model = self.blip_model.to(\"cpu\")\n",
        "        return caption[0]\n",
        "\n",
        "    def image_to_features(self, image: Image) -> torch.Tensor:\n",
        "        images = self.clip_preprocess(image).unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            image_features = self.clip_model.encode_image(images)\n",
        "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        return image_features\n",
        "\n",
        "    \n",
        "    def interrogate(self, caption: str, image: Image) -> str:\n",
        "        image_features = self.image_to_features(image)\n",
        "\n",
        "        # flaves = self.flavors.rank(image_features, self.config.flavor_intermediate_count)\n",
        "        # best_medium = self.mediums.rank(image_features, 1)[0]\n",
        "        # best_artist = self.artists.rank(image_features, 1)[0]\n",
        "        # best_trending = self.trendings.rank(image_features, 1)[0]\n",
        "        # best_movement = self.movements.rank(image_features, 1)[0]\n",
        "\n",
        "        best_prompt = caption\n",
        "        best_sim = self.similarity(image_features, best_prompt)\n",
        "\n",
        "        def check(addition: str) -> bool:\n",
        "            nonlocal best_prompt, best_sim\n",
        "            prompt = best_prompt + \", \" + addition\n",
        "            sim = self.similarity(image_features, prompt)\n",
        "            if sim > best_sim:\n",
        "                best_sim = sim\n",
        "                best_prompt = prompt\n",
        "                return True\n",
        "            return False\n",
        "\n",
        "        def check_multi_batch(opts: List[str]):\n",
        "            nonlocal best_prompt, best_sim\n",
        "            prompts = []\n",
        "            for i in range(2**len(opts)):\n",
        "                prompt = best_prompt\n",
        "                for bit in range(len(opts)):\n",
        "                    if i & (1 << bit):\n",
        "                        prompt += \", \" + opts[bit]\n",
        "                prompts.append(prompt)\n",
        "\n",
        "            t = LabelTable(prompts, None, self.clip_model, self.tokenize, self.config)\n",
        "            best_prompt = t.rank(image_features, 1)[0]\n",
        "            best_sim = self.similarity(image_features, best_prompt)\n",
        "\n",
        "        batch = []\n",
        "\n",
        "        if clip_use_artist:\n",
        "            batch.append(self.artists.rank(image_features,1)[0])\n",
        "        if clip_use_flavor:\n",
        "                best_flavors = self.flavors.rank(image_features, self.config.flavor_intermediate_count)\n",
        "                extended_flavors = set(best_flavors)\n",
        "                for _ in tqdm(range(clip_max_flavors), desc=\"Flavor chain\", disable=self.config.quiet):\n",
        "                    best = self.rank_top(image_features, [f\"{best_prompt}, {f}\" for f in extended_flavors])\n",
        "                    flave = best[len(best_prompt) + 2:]\n",
        "                    if not check(flave):\n",
        "                        break\n",
        "                    if _prompt_at_max_len(best_prompt, self.tokenize):\n",
        "                        break\n",
        "                    extended_flavors.remove(flave)\n",
        "        if clip_use_medium:\n",
        "            batch.append(self.mediums.rank(image_features, 1)[0])\n",
        "        if clip_use_trending:\n",
        "            batch.append(self.trendings.rank(image_features, 1)[0])\n",
        "        if clip_use_movement:\n",
        "            batch.append(self.movements.rank(image_features, 1)[0])\n",
        "\n",
        "        check_multi_batch(batch)\n",
        "        tags = best_prompt.split(\",\")\n",
        "\n",
        "        return tags\n",
        "        # check_multi_batch([best_medium, best_artist, best_trending, best_movement])\n",
        "\n",
        "        # extended_flavors = set(flaves)\n",
        "        # for _ in tqdm(range(max_flavors), desc=\"Flavor chain\", disable=self.config.quiet):\n",
        "        #     best = self.rank_top(image_features, [f\"{best_prompt}, {f}\" for f in extended_flavors])\n",
        "        #     flave = best[len(best_prompt)+2:]\n",
        "        #     if not check(flave):\n",
        "        #         break\n",
        "        #     if _prompt_at_max_len(best_prompt, self.tokenize):\n",
        "        #         break\n",
        "        #     extended_flavors.remove(flave)\n",
        "\n",
        "        # return best_prompt\n",
        "\n",
        "    def rank_top(self, image_features: torch.Tensor, text_array: List[str]) -> str:\n",
        "        text_tokens = self.tokenize([text for text in text_array]).to(self.device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            text_features = self.clip_model.encode_text(text_tokens)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = text_features @ image_features.T\n",
        "        return text_array[similarity.argmax().item()]\n",
        "\n",
        "    def similarity(self, image_features: torch.Tensor, text: str) -> float:\n",
        "        text_tokens = self.tokenize([text]).to(self.device)\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "            text_features = self.clip_model.encode_text(text_tokens)\n",
        "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "            similarity = text_features @ image_features.T\n",
        "        return similarity[0][0].item()\n",
        "\n",
        "\n",
        "class LabelTable():\n",
        "    def __init__(self, labels:List[str], desc:str, clip_model, tokenize, config: Config):\n",
        "        self.chunk_size = config.chunk_size\n",
        "        self.config = config\n",
        "        self.device = config.device\n",
        "        self.embeds = []\n",
        "        self.labels = labels\n",
        "        self.tokenize = tokenize\n",
        "\n",
        "        hash = hashlib.sha256(\",\".join(labels).encode()).hexdigest()\n",
        "\n",
        "        cache_filepath = None\n",
        "        if config.cache_path is not None and desc is not None:\n",
        "            os.makedirs(config.cache_path, exist_ok=True)\n",
        "            sanitized_name = config.clip_model_name.replace('/', '_').replace('@', '_')\n",
        "            cache_filepath = os.path.join(config.cache_path, f\"{sanitized_name}_{desc}.pkl\")\n",
        "            if desc is not None and os.path.exists(cache_filepath):\n",
        "                with open(cache_filepath, 'rb') as f:\n",
        "                    try:\n",
        "                        data = pickle.load(f)\n",
        "                        if data.get('hash') == hash:\n",
        "                            self.labels = data['labels']\n",
        "                            self.embeds = data['embeds']\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading cached table {desc}: {e}\")\n",
        "\n",
        "        if len(self.labels) != len(self.embeds):\n",
        "            self.embeds = []\n",
        "            chunks = np.array_split(self.labels, max(1, len(self.labels)/config.chunk_size))\n",
        "            for chunk in tqdm(chunks, desc=f\"Preprocessing {desc}\" if desc else None, disable=self.config.quiet):\n",
        "                text_tokens = self.tokenize(chunk).to(self.device)\n",
        "                with torch.no_grad(), torch.cuda.amp.autocast():\n",
        "                    text_features = clip_model.encode_text(text_tokens)\n",
        "                    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "                    text_features = text_features.half().cpu().numpy()\n",
        "                for i in range(text_features.shape[0]):\n",
        "                    self.embeds.append(text_features[i])\n",
        "\n",
        "            if cache_filepath is not None:\n",
        "                with open(cache_filepath, 'wb') as f:\n",
        "                    pickle.dump({\n",
        "                        \"labels\": self.labels, \n",
        "                        \"embeds\": self.embeds, \n",
        "                        \"hash\": hash, \n",
        "                        \"model\": config.clip_model_name\n",
        "                    }, f)\n",
        "\n",
        "        if self.device == 'cpu' or self.device == torch.device('cpu'):\n",
        "            self.embeds = [e.astype(np.float32) for e in self.embeds]\n",
        "    \n",
        "    def _rank(self, image_features: torch.Tensor, text_embeds: torch.Tensor, top_count: int=1) -> str:\n",
        "        top_count = min(top_count, len(text_embeds))\n",
        "        text_embeds = torch.stack([torch.from_numpy(t) for t in text_embeds]).to(self.device)\n",
        "        with torch.cuda.amp.autocast():\n",
        "            similarity = image_features @ text_embeds.T\n",
        "        _, top_labels = similarity.float().cpu().topk(top_count, dim=-1)\n",
        "        return [top_labels[0][i].numpy() for i in range(top_count)]\n",
        "\n",
        "    def rank(self, image_features: torch.Tensor, top_count: int=1) -> List[str]:\n",
        "        if len(self.labels) <= self.chunk_size:\n",
        "            tops = self._rank(image_features, self.embeds, top_count=top_count)\n",
        "            return [self.labels[i] for i in tops]\n",
        "\n",
        "        num_chunks = int(math.ceil(len(self.labels)/self.chunk_size))\n",
        "        keep_per_chunk = int(self.chunk_size / num_chunks)\n",
        "\n",
        "        top_labels, top_embeds = [], []\n",
        "        for chunk_idx in tqdm(range(num_chunks), disable=self.config.quiet):\n",
        "            start = chunk_idx*self.chunk_size\n",
        "            stop = min(start+self.chunk_size, len(self.embeds))\n",
        "            tops = self._rank(image_features, self.embeds[start:stop], top_count=keep_per_chunk)\n",
        "            top_labels.extend([self.labels[start+i] for i in tops])\n",
        "            top_embeds.extend([self.embeds[start+i] for i in tops])\n",
        "\n",
        "        tops = self._rank(image_features, top_embeds, top_count=top_count)\n",
        "        return [top_labels[i] for i in tops]\n",
        "\n",
        "\n",
        "def _load_list(data_path: str, filename: str) -> List[str]:\n",
        "    with open(os.path.join(data_path, filename), 'r', encoding='utf-8', errors='replace') as f:\n",
        "        items = [line.strip() for line in f.readlines()]\n",
        "    return items\n",
        "\n",
        "def _merge_tables(tables: List[LabelTable], config: Config) -> LabelTable:\n",
        "    m = LabelTable([], None, None, None, config)\n",
        "    for table in tables:\n",
        "        m.labels.extend(table.labels)\n",
        "        m.embeds.extend(table.embeds)\n",
        "    return m\n",
        "\n",
        "def _prompt_at_max_len(text: str, tokenize) -> bool:\n",
        "    tokens = tokenize([text])\n",
        "    return tokens[0][-1] != 0\n",
        "\n",
        "def _truncate_to_fit(text: str, tokenize) -> str:\n",
        "    parts = text.split(', ')\n",
        "    new_text = parts[0]\n",
        "    for part in parts[1:]:\n",
        "        if _prompt_at_max_len(new_text + part, tokenize):\n",
        "            break\n",
        "        new_text += ', ' + part\n",
        "    return new_text\n",
        "\n",
        "ci:Interrogator = None\n",
        "\n",
        "        \n",
        "def process_img(img_path):\n",
        "    # Load image\n",
        "    with Image.open(img_path).convert('RGB') as img:\n",
        "\n",
        "      # Get existing caption\n",
        "      existing_caption = ''\n",
        "      cap_file = os.path.join(folder_path,os.path.splitext(os.path.split(img_path)[1])[0] + '.txt')\n",
        "      if os.path.isfile(cap_file):\n",
        "        with open(cap_file) as f:\n",
        "          existing_caption = f.read()\n",
        "\n",
        "      # Get caption from filename if empty\n",
        "      if existing_caption == '' and use_filename:\n",
        "          path = os.path.split(img_path)[1]\n",
        "          path = os.path.splitext(path)[0]\n",
        "          existing_caption = ''.join(c for c in path if c.isalpha() or c in [\" \", \",\"])\n",
        "      \n",
        "      \n",
        "      # Create tag list\n",
        "      out_tags = []\n",
        "      new_caption = ''\n",
        "    \n",
        "      # 1st caption pass: GIT\n",
        "      if git_pass:\n",
        "          new_caption = git_caption(img)\n",
        "          print('Got git caption: ',new_caption)\n",
        "          # Check if caption fails from list of not-allowed phrases\n",
        "          if blip_pass and any(f in new_caption for f in git_fail_phrases.split(',')):\n",
        "              # Fail git caption\n",
        "              new_caption = ''\n",
        "\n",
        "      # 2nd caption pass: BLIP (if failed)\n",
        "      if blip_pass and new_caption == '':\n",
        "          new_caption = ci.generate_blip_caption(img)\n",
        "\n",
        "\n",
        "\n",
        "      # Add enabled CLIP flavors to tag list\n",
        "      if clip_use_artist or clip_use_flavor or clip_use_medium or clip_use_movement or clip_use_trending:\n",
        "          tags = ci.interrogate(new_caption,img)\n",
        "          for tag in tags:\n",
        "              out_tags.append(tag)\n",
        "      else:\n",
        "          for tag in new_caption.split(\",\"):\n",
        "              out_tags.append(tag)\n",
        "\n",
        "\n",
        "      # Add parent folder to tag list if enabled\n",
        "      if folder_tag:\n",
        "          folder_tags = get_parent_folder(img_path,folder_tag_levels)\n",
        "          for tag in folder_tags:\n",
        "              out_tags.append(tag)\n",
        "\n",
        "      # Remove duplicates, filter dumb stuff\n",
        "      # chars_to_strip = [\"_\\\\(\"]\n",
        "      unique_tags = []\n",
        "      tags_to_ignore = []\n",
        "      if ignore_tags != \"\" and ignore_tags is not None:\n",
        "          si_tags = ignore_tags.split(\",\")\n",
        "          for tag in si_tags:\n",
        "              tags_to_ignore.append(tag.strip)\n",
        "\n",
        "      if uniquify_tags:\n",
        "          for tag in out_tags:\n",
        "              if not tag in unique_tags and not \"_\\(\" in tag and not tag in ignore_tags:\n",
        "                  unique_tags.append(tag.strip())\n",
        "      else:\n",
        "          for tag in out_tags:\n",
        "              if not \"_\\(\" in tag and not tag in ignore_tags:\n",
        "                  unique_tags.append(tag.strip())\n",
        "\n",
        "      existing_tags = existing_caption.split(\",\")\n",
        "\n",
        "      # APPEND/PREPEND/OVERWRITE existing caption based on options\n",
        "      if existing == \"prepend\" and len(existing_tags):\n",
        "          new_tags = existing_tags\n",
        "          for tag in unique_tags:\n",
        "              if not tag in new_tags or not uniquify_tags:\n",
        "                  new_tags.append(tag)\n",
        "          unique_tags = new_tags\n",
        "\n",
        "      if existing == 'append' and len(existing_tags):\n",
        "          for tag in existing_tags:\n",
        "              if not tag in unique_tags or not uniquify_tags:\n",
        "                  unique_tags.append(tag)\n",
        "\n",
        "      if existing == 'copy' and existing_caption:\n",
        "          for tag in existing_tags:\n",
        "              unique_tags.append(tag.strip())\n",
        "\n",
        "      # Construct new caption from tag list\n",
        "      caption_txt = \", \".join(unique_tags)\n",
        "\n",
        "      if replace_class and sub_name is not None and sub_class is not None:\n",
        "          # Find and replace \"a SUBJECT CLASS\" in caption_txt with subject name\n",
        "          if f\"a {sub_class}\" in caption_txt:\n",
        "              caption_txt = caption_txt.replace(f\"a {sub_class}\", sub_name)\n",
        "\n",
        "          if sub_class in caption_txt:\n",
        "              caption_txt = caption_txt.replace(sub_class, sub_name)\n",
        "\n",
        "      tags = caption_txt.split(\" \")\n",
        "      if cap_length != 0 and len(tags) > cap_length:\n",
        "              tags = tags[0:cap_length]\n",
        "              tags[-1] = tags[-1].rstrip(\",\")\n",
        "      caption_txt = \" \".join(tags)\n",
        "\n",
        "      # Write caption file\n",
        "      if write_to_file:\n",
        "          with open(os.path.join(output_path,cap_file), \"w\", encoding=\"utf8\") as file:\n",
        "                      file.write(caption_txt)\n",
        "                      print(f'Wrote {cap_file}')\n",
        "\n",
        "      display(HTML(f'<h3>{caption_txt}</h3>'))\n",
        "      display(img.resize((200,200)))\n",
        "\n",
        "\n",
        "\n",
        "processor = None\n",
        "model = None\n",
        "ci = None\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "##### INIT HERE. SEND --READY-- WHEN READY FOR INPUT\n",
        "if git_pass:\n",
        "    processor = AutoProcessor.from_pretrained(\"microsoft/git-large-r-textcaps\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-large-r-textcaps\")\n",
        "    model.to(device)\n",
        "\n",
        "if clip_use_movement or clip_use_artist or clip_use_flavor or clip_use_medium or clip_use_trending or blip_pass:\n",
        "    ci = Interrogator(Config(clip_model_name='ViT-L-14/openai' if not clip_v2 else 'coca_ViT-L-14/mscoco_finetuned_laion2B-s13B-b90k',\n",
        "                          quiet=False))\n",
        "\n",
        "\n",
        "for root, dirs, files in os.walk(folder_path, topdown=False):\n",
        "   for name in files:\n",
        "     if 'txt' not in os.path.splitext(os.path.split(name)[1])[1]:\n",
        "      cap_file = os.path.join(folder_path,os.path.splitext(os.path.split(name)[1])[0] + '.txt')\n",
        "      if not existing == 'skip' or not os.path.exists(cap_file):\n",
        "        process_img(os.path.join(root, name))\n",
        "      else:\n",
        "        print(f'Caption file {cap_file} exists. Skipping.')\n",
        "\n",
        "processor = None\n",
        "model = None\n",
        "ci = None\n",
        "with torch.no_grad():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define your Huggingface Repo\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "%store -r\n",
        "\n",
        "api = HfApi()\n",
        "user = api.whoami(write_token)\n",
        "\n",
        "#@markdown #### If your model/dataset repo doesn't exist, it will automatically create your repo.\n",
        "dataset_name = \"to8spirit-data\" #@param{type:\"string\"}\n",
        "make_this_dataset_private = True #@param{type:\"boolean\"}\n",
        "\n",
        "datasets_repo = user['name']+\"/\"+dataset_name.strip()\n",
        "\n",
        "if dataset_name != \"\":\n",
        "  try:\n",
        "      validate_repo_id(datasets_repo)\n",
        "      api.create_repo(repo_id=datasets_repo,\n",
        "                      repo_type=\"dataset\",\n",
        "                      private=make_this_dataset_private)\n",
        "      print(\"Dataset Repo didn't exists, creating repo\")\n",
        "      print(\"Dataset Repo\",datasets_repo,\"created!\\n\")\n",
        "\n",
        "  except HfHubHTTPError as e:\n",
        "      print(f\"Dataset repo: {datasets_repo} exists, skipping create repo\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "F06v8KVh7m7j",
        "outputId": "1ec9e7a1-a501-49f3-ba0c-c4d15cc04474"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset repo: theovercomer8/to8spirit-data exists, skipping create repo\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Upload Dataset\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "#@markdown #### This will be compressed to zip and  uploaded to datasets repo, leave it empty if not necessary\n",
        "dataset_path = \"/content/dataset\" #@param {type :\"string\"}\n",
        "dataset_name = \"dataset2\" #@param {type :\"string\"}\n",
        "#@markdown #### Delete zip after upload\n",
        "delete_zip = True #@param {type :\"boolean\"}\n",
        "\n",
        "tmp_dataset = \"/content/dataset\"\n",
        "\n",
        "dataset_zip = f\"/content/{dataset_name}.zip\"\n",
        "\n",
        "\n",
        "#@markdown #### Other Information\n",
        "commit_message = \"\" #@param {type :\"string\"}\n",
        "\n",
        "if not commit_message:\n",
        "  commit_message = \"feat: upload captioned dataset\"\n",
        "\n",
        "def upload_dataset(dataset_paths, is_zip : bool):\n",
        "  path_obj = Path(dataset_paths)\n",
        "  dataset_name = path_obj.parts[-1]\n",
        "\n",
        "  if is_zip:\n",
        "    print(f\"Uploading dataset to https://huggingface.co/datasets/\"+datasets_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=dataset_paths,\n",
        "        path_in_repo=dataset_name,\n",
        "        repo_id=datasets_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        commit_message=commit_message,\n",
        "    )\n",
        "    print(f\"Upload success, located at https://huggingface.co/datasets/\"+datasets_repo+\"/blob/main/\"+dataset_name+\"\\n\")\n",
        "  else:\n",
        "    print(f\"Uploading {dataset_name} to https://huggingface.co/datasets/\"+datasets_repo)\n",
        "    print(f\"Please wait...\")\n",
        "\n",
        "    api.upload_folder(\n",
        "        folder_path=dataset_paths,\n",
        "        path_in_repo=dataset_name,\n",
        "        repo_id=datasets_repo,\n",
        "        repo_type=\"dataset\",\n",
        "        commit_message=commit_message,\n",
        "        ignore_patterns=\".ipynb_checkpoints\",\n",
        "    )\n",
        "    print(f\"Upload success, located at https://huggingface.co/datasets/\"+datasets_repo+\"/tree/main/\"+dataset_name+\"\\n\")\n",
        "  \n",
        "def zip_file(tmp,zip):\n",
        "    zipfiles = zip \n",
        "    with zipfile.ZipFile(zipfiles, 'w') as zip:\n",
        "      for tmp, dirs, files in os.walk(tmp):\n",
        "          for file in files:\n",
        "              zip.write(os.path.join(tmp, file))\n",
        "\n",
        "def upload():\n",
        "  zip_file(tmp_dataset,dataset_zip)\n",
        "  upload_dataset(dataset_zip, True)\n",
        "  if delete_zip:\n",
        "    os.remove(dataset_zip)\n",
        "\n",
        "upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "dUKhs0SB60VE",
        "outputId": "d5c51601-c255-484a-9030-a98c2ed7aae9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading dataset to https://huggingface.co/datasets/theovercomer8/to8spirit-data\n",
            "Please wait...\n",
            "Upload success, located at https://huggingface.co/datasets/theovercomer8/to8spirit-data/blob/main/dataset2.zip\n",
            "\n"
          ]
        }
      ]
    }
  ]
}